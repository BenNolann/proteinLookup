---
title: "proteinLookup: Data download and SQL dump creation"
output:
  html_document:
    df_print: paged
date: "2023-05-15"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Obtain both tables from ChIP-atlas

### Command Line

Download data from ChIP-atlas for all hg38 experiments with a significance cutoff of Q < 1e-50. (most stringent).

```{bash, eval=F}
wget https://chip-atlas.dbcls.jp/data/hg38/allPeaks_light/allPeaks_light.hg38.50.bed.gz
```

Unzip the data

* Details:
  + Lines = 282,806,136
  + Size  = 10Gb

```{bash, eval=F}
gunzip allPeaks_light.hg38.50.bed.gz
```

Download data from ChIP-atlas for all ChIP-seq experiments

```{bash, eval=F}
wget https://chip-atlas.dbcls.jp/data/metadata/experimentList.tab
```

Take the first 1000 experiments

```{bash, eval=F}
head experimentList.tab -n 1000 > experimentList1000lines.tab
```

Sort peaks file by study_id. 

**NOTE:** This takes a while...

```{bash, eval=F}
sort -k4,4 allPeaks_light.hg38.50.bed > allPeaks_light.hg38.50.sorted.bed
```

## Grab peaks data for 1000 experiments

### Python

Python script to grab peaks lines until 1001st cutoff. This relies on both files being ordered by study_id.

```{python, eval=F}
#!/usr/bin/env python3

# Read in sorted files
exp = "experimentList1000lines.tab"
peak = "allPeaks_light.hg38.50.sorted.bed"


# Using experiment as a cutoff (n-1 experiments will remain)
# Print out all peaks until then
cutoff = 'DRX127823' #1001st entry

with open(peak) as fp:
    for line in fp:
        elem = line.split('\t')
        if(elem[3] == cutoff ):
            break
        print(line.rstrip())

```


The all peaks file can be generated by running:

```{bash, eval=F}
python <above-script>.py > allpeaks1000experiments.bed
```

#### Remove all quotes from metadata file.

This is necessary for the sql dump to function properly.

```{bash, eval=F}
sed -i 's/"//g' experimentList1000lines.tab
```


## Create an SQL dump from experiments and peaks files

Use python to create **INSERT** statements to populate SQL database. A proper SQL dump file can be made from within MySQL once the database is created.

```{python, eval=F}
#!/usr/bin/env python3

print(
"""
DROP TABLE IF EXISTS all_samples_peaks;

CREATE TABLE all_samples_peaks(
    chr VARCHAR(100),
    start INT,
    stop INT,
    strength INT,
    study_id VARCHAR(50),
    PRIMARY KEY(chr,start,stop,study_id));
"""
)

data = "allpeaks1000experiments.bed"

with open(data) as fp:
    for line in fp:
        elements = line.split("\t")
        print('INSERT INTO all_samples_peaks VALUES("' + 
              elements[0] + '","' +
              elements[1] + '","' +
              elements[2] + '","' +
              elements[4].rstrip() + '","' +
              elements[3] + '");'
              )
        


print(
"""
DROP TABLE IF EXISTS all_samples_experiments;

CREATE TABLE all_samples_experiments(
    study_id VARCHAR(50),
    antibody VARCHAR(50),
    tissue VARCHAR(50),
    cell_type VARCHAR(50),
    description VARCHAR(10000),
    PRIMARY KEY(study_id)
);
"""
)

data2 = "experimentList1000lines.tab"

with open(data2) as fp:
    for line in fp:
        elements = line.split("\t")
        print('INSERT INTO all_samples_experiments VALUES("' + 
              elements[0] + '","' +
              elements[3] + '","' +
              elements[4] + '","' +
              elements[5] + '","' +
              elements[6] + '");'
              )

```


```{bash, eval=F}
python <sql-dump-script.py> > proteinlookup.sql
```










